{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17fdb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of GPT from scratch (but charcter level) and train it on tiny shaksphere\n",
    "\n",
    "# 2017, Attention is all you need \n",
    "# GPT -> Generatively Pretrained Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59716f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", 'r', encoding='utf-8')as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25907454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of charcters = 1115394\n",
      "no of line= 40000\n"
     ]
    }
   ],
   "source": [
    "print(f\"no of charcters = {len(text)}\")\n",
    "print(f\"no of line= {len(text.splitlines())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bfdc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a50fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n"
     ]
    }
   ],
   "source": [
    "# get all characters \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f\"vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6a438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from chars to int and vise-versa\n",
    "# character level tokeniser \n",
    "# long sequences \n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "# How lambda works--> some_fucntion_object = lambda input:output_(input)\n",
    "encode = lambda  s:[stoi[c] for c in s] #encoder: take a string, output a list of integer\n",
    "decode = lambda  l: ''.join([itos[i] for i in l]) # decode: takes a list of indexes and output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e493178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "hello world!\n",
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "# test the encoder and decoder \n",
    "print(encode(\"hello world!\"))\n",
    "print(decode([46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]))\n",
    "print(decode(encode(\"hello world!\"))) # if it prints \"hello world!\" it works !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef58586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584bbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation split \n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c55f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when the context is tensor([18]) ->target is 47\n",
      "when the context is ['F'] ->target is 'i'\n",
      "---------\n",
      "when the context is tensor([18, 47]) ->target is 56\n",
      "when the context is ['F', 'i'] ->target is 'r'\n",
      "---------\n",
      "when the context is tensor([18, 47, 56]) ->target is 57\n",
      "when the context is ['F', 'i', 'r'] ->target is 's'\n",
      "---------\n",
      "when the context is tensor([18, 47, 56, 57]) ->target is 58\n",
      "when the context is ['F', 'i', 'r', 's'] ->target is 't'\n",
      "---------\n",
      "when the context is tensor([18, 47, 56, 57, 58]) ->target is 1\n",
      "when the context is ['F', 'i', 'r', 's', 't'] ->target is ' '\n",
      "---------\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1]) ->target is 15\n",
      "when the context is ['F', 'i', 'r', 's', 't', ' '] ->target is 'C'\n",
      "---------\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15]) ->target is 47\n",
      "when the context is ['F', 'i', 'r', 's', 't', ' ', 'C'] ->target is 'i'\n",
      "---------\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]) ->target is 58\n",
      "when the context is ['F', 'i', 'r', 's', 't', ' ', 'C', 'i'] ->target is 't'\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# when the input is {context} the target is {target}\n",
    "block_size = 8 # time dimention\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    context_itos = [itos[i.item()] for i in context]\n",
    "    target = y[t]\n",
    "    target_itos = itos[target.item()]\n",
    "    print(f\"when the context is {context} ->target is {target}\")\n",
    "    print(f\"when the context is {context_itos} ->target is '{target_itos}'\")\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c16a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "torch.Size([4, 8])\n",
      "---------\n",
      "printing sequence:1, example: 1\n",
      "when the context is ['o'] ->target is 'u'\n",
      "---------\n",
      "printing sequence:1, example: 2\n",
      "when the context is ['o', 'u'] ->target is ','\n",
      "---------\n",
      "printing sequence:1, example: 3\n",
      "when the context is ['o', 'u', ','] ->target is ' '\n",
      "---------\n",
      "printing sequence:1, example: 4\n",
      "when the context is ['o', 'u', ',', ' '] ->target is 't'\n",
      "---------\n",
      "printing sequence:1, example: 5\n",
      "when the context is ['o', 'u', ',', ' ', 't'] ->target is 'r'\n",
      "---------\n",
      "printing sequence:1, example: 6\n",
      "when the context is ['o', 'u', ',', ' ', 't', 'r'] ->target is 'i'\n",
      "---------\n",
      "printing sequence:1, example: 7\n",
      "when the context is ['o', 'u', ',', ' ', 't', 'r', 'i'] ->target is 'b'\n",
      "---------\n",
      "printing sequence:1, example: 8\n",
      "when the context is ['o', 'u', ',', ' ', 't', 'r', 'i', 'b'] ->target is 'u'\n",
      "---------\n",
      "printing sequence:2, example: 1\n",
      "when the context is ['k'] ->target is 'e'\n",
      "---------\n",
      "printing sequence:2, example: 2\n",
      "when the context is ['k', 'e'] ->target is 'e'\n",
      "---------\n",
      "printing sequence:2, example: 3\n",
      "when the context is ['k', 'e', 'e'] ->target is 'p'\n",
      "---------\n",
      "printing sequence:2, example: 4\n",
      "when the context is ['k', 'e', 'e', 'p'] ->target is ' '\n",
      "---------\n",
      "printing sequence:2, example: 5\n",
      "when the context is ['k', 'e', 'e', 'p', ' '] ->target is 'i'\n",
      "---------\n",
      "printing sequence:2, example: 6\n",
      "when the context is ['k', 'e', 'e', 'p', ' ', 'i'] ->target is 't'\n",
      "---------\n",
      "printing sequence:2, example: 7\n",
      "when the context is ['k', 'e', 'e', 'p', ' ', 'i', 't'] ->target is ' '\n",
      "---------\n",
      "printing sequence:2, example: 8\n",
      "when the context is ['k', 'e', 'e', 'p', ' ', 'i', 't', ' '] ->target is 't'\n",
      "---------\n",
      "printing sequence:3, example: 1\n",
      "when the context is ['A'] ->target is 'n'\n",
      "---------\n",
      "printing sequence:3, example: 2\n",
      "when the context is ['A', 'n'] ->target is 'g'\n",
      "---------\n",
      "printing sequence:3, example: 3\n",
      "when the context is ['A', 'n', 'g'] ->target is 'e'\n",
      "---------\n",
      "printing sequence:3, example: 4\n",
      "when the context is ['A', 'n', 'g', 'e'] ->target is 'l'\n",
      "---------\n",
      "printing sequence:3, example: 5\n",
      "when the context is ['A', 'n', 'g', 'e', 'l'] ->target is 'o'\n",
      "---------\n",
      "printing sequence:3, example: 6\n",
      "when the context is ['A', 'n', 'g', 'e', 'l', 'o'] ->target is '.'\n",
      "---------\n",
      "printing sequence:3, example: 7\n",
      "when the context is ['A', 'n', 'g', 'e', 'l', 'o', '.'] ->target is '\n",
      "'\n",
      "---------\n",
      "printing sequence:3, example: 8\n",
      "when the context is ['A', 'n', 'g', 'e', 'l', 'o', '.', '\\n'] ->target is 'N'\n",
      "---------\n",
      "printing sequence:4, example: 1\n",
      "when the context is [' '] ->target is 'a'\n",
      "---------\n",
      "printing sequence:4, example: 2\n",
      "when the context is [' ', 'a'] ->target is ' '\n",
      "---------\n",
      "printing sequence:4, example: 3\n",
      "when the context is [' ', 'a', ' '] ->target is 'h'\n",
      "---------\n",
      "printing sequence:4, example: 4\n",
      "when the context is [' ', 'a', ' ', 'h'] ->target is 'o'\n",
      "---------\n",
      "printing sequence:4, example: 5\n",
      "when the context is [' ', 'a', ' ', 'h', 'o'] ->target is 'u'\n",
      "---------\n",
      "printing sequence:4, example: 6\n",
      "when the context is [' ', 'a', ' ', 'h', 'o', 'u'] ->target is 's'\n",
      "---------\n",
      "printing sequence:4, example: 7\n",
      "when the context is [' ', 'a', ' ', 'h', 'o', 'u', 's'] ->target is 'e'\n",
      "---------\n",
      "printing sequence:4, example: 8\n",
      "when the context is [' ', 'a', ' ', 'h', 'o', 'u', 's', 'e'] ->target is '\n",
      "'\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Batch dimention\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 8 # what is the length of each sequence \n",
    "batch_size = 4 # how many independent sequences we will process in parallel \n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Get a mini batch from specified data split\n",
    "    input: \"train\" or \"val\"\n",
    "    output: x, y \n",
    "    y is x moved to right by an index \n",
    "    x.shape -> [4,8]\n",
    "    yb.shape -> [4,8]\n",
    "    \"\"\"\n",
    "    data = train_data if split ==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size-1,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb)\n",
    "print(xb.shape)\n",
    "print(yb)\n",
    "print(yb.shape)\n",
    "\n",
    "print(\"---------\")\n",
    "# -----\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        context_itos = [itos[i.item()] for i in context]\n",
    "        target = yb[b,t]\n",
    "        target_itos = itos[target.item()]\n",
    "        print(f\"printing sequence:{b+1}, example: {t+1}\")\n",
    "        print(f\"when the context is {context_itos} ->target is '{target_itos}'\")\n",
    "        print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498633a",
   "metadata": {},
   "source": [
    "### Biagram Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a072e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.894842624664307\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# nn.Module -> Base class for all neural network modules.\n",
    "\n",
    "class BiagramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits of the next token from a look up table\n",
    "        # nn.Embedding -> A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx,targets=None):\n",
    "        # idx and targets are of shape (B,T)\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            # pytorch cross entropy expects second dimention as the channel\n",
    "            logits = logits.view(B*T , C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B,T array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx) # (B, T, C) # self here runs the def forward \n",
    "            # get predictions for the last time step \n",
    "            logits = logits[:, -1, :] #(B,C) pluck the last time step\n",
    "            # apply soft max to get the probabilities \n",
    "            prob = F.softmax(logits, dim = -1) #(B, C)\n",
    "            # sample from the distibution \n",
    "            idx_next = torch.multinomial(prob, num_samples=1) # (B, 1)\n",
    "            # append the sampled index into the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim= 1) # (B,T+1)\n",
    "        return idx \n",
    "m = BiagramLanguageModel(vocab_size)\n",
    "logits,loss = m (xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "\n",
    "# generate and decode \n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9495c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch optimization object \n",
    "# a good learnign rate is 1e-4, but for small networks we can use 1e-3\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5db8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.464529514312744\n"
     ]
    }
   ],
   "source": [
    "# training loop \n",
    "batch_size = 32 \n",
    "for _ in range(10000):\n",
    "    # sample from the batch\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # forward pass and evaluate the loss \n",
    "    logits, loss = m(xb,yb)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # update \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88d5d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tond cks,\n",
      "Themevelviesthe'; t:\n",
      "YCURYORKI os it a frimowhion'edeteistink cey weron le'theayolllefovil\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f65506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was a very simple model, \n",
    "# even though we are feeding in the context we only look at the last character \n",
    "# the tokens are not talking to each other "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74295a84",
   "metadata": {},
   "source": [
    "### Maths of self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2c81c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 6,4, 2 # batch, time and channel \n",
    "x = torch.randn(B,T,C)\n",
    "xbow = torch.zeros(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc467dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging example \n",
    "# version. 1 \n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b,:t+1] #T, C \n",
    "        xbow[b,t] = torch.mean(x_prev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A, B , C example \n",
    "A = torch.tril(torch.ones(3,3))\n",
    "A = A / A.sum(1, keepdim=True)\n",
    "B = torch.randint(1,10, (3,2)).float()\n",
    "C = A@B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "279a764d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply matrix multiplication to averaging example \n",
    "# version 2 \n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(1, keepdim=True) #(4,4)\n",
    "xbow2 = wei @ x # (T,T) @ (B,T,T) => (B,T,T) @ (B,T,C) => (B,T,C)\n",
    "torch.allclose(xbow2, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3 uses softmax \n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim =-1 )\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec95114",
   "metadata": {},
   "source": [
    "## Starter code for making Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f5e01e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of charcters = 1115394\n",
      "No of line = 40000\n",
      "unique character: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size: 65\n",
      "Length of train data: 1003854\n",
      "Length of validation data: 111540\n",
      "model created successfully!\n",
      "training started...\n",
      "step 0/ 3000,  train_loss: 4.4788, val_loss: 4.4784\n",
      "step 300/ 3000,  train_loss: 2.5474, val_loss: 2.5587\n",
      "step 600/ 3000,  train_loss: 2.5103, val_loss: 2.5461\n",
      "step 900/ 3000,  train_loss: 2.4930, val_loss: 2.5300\n",
      "step 1200/ 3000,  train_loss: 2.4883, val_loss: 2.5196\n",
      "step 1500/ 3000,  train_loss: 2.4944, val_loss: 2.5138\n",
      "step 1800/ 3000,  train_loss: 2.5044, val_loss: 2.5218\n",
      "step 2100/ 3000,  train_loss: 2.5001, val_loss: 2.5068\n",
      "step 2400/ 3000,  train_loss: 2.4977, val_loss: 2.5331\n",
      "step 2700/ 3000,  train_loss: 2.4962, val_loss: 2.4944\n",
      "training complete!\n",
      "\n",
      "\n",
      "Stoulowerefrthicaprorheellyouesin.\n",
      "\n",
      "VERangrey pe My f callereed jeay\n",
      "\n",
      "Th kiss fouthe t-\n",
      "\n",
      "s t\n",
      "\n",
      "RIAndithy thtand pursthare ce gres ughachicalicef ',\n",
      "The, thean h ure ndrdyousolfive sseaik!\n",
      "Where\n",
      "Blinet s, ts busthen hye.\n",
      "ONGLol s.\n",
      "\n",
      "Shranl tirofou will t.\n",
      "\n",
      "Whey mor.\n",
      "\n",
      "BELLLa?\n",
      "NCALENThetowe ke win thind confencllcheag r's:\n",
      "GHerimatofeerequtheth iusme my hice lly t ont Gon avevethilo!\n",
      "Thesharow l, ang shar prntthoklarindben QURe s as Isty, wom thind henjonberesthawosh y\n",
      "AI, crerenohis t wglist,\n",
      "AULIU\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Script to implement language model\"\"\"\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# hyperparameters \n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "n_embd = 32 # no of embedding dimentions\n",
    "max_iterations = 3000\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200\n",
    "eval_interval = 300 \n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# -------------------------------------\n",
    "\n",
    "# read the data \n",
    "with open(\"/Users/rahulkrish/Desktop/myrepos/deeplearning/data/input.txt\", \"r\", encoding= 'utf-8')as f:\n",
    "    text = f.read()\n",
    "print(f\"No of charcters = {len(text)}\")\n",
    "print(f\"No of line = {len(text.splitlines())}\")\n",
    "\n",
    "# identify vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"unique character: {''.join(chars)}\")\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "# create a mapping from chars to int and vise-versa\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "\n",
    "# encoder and decoder \n",
    "encode = lambda  s:[stoi[c] for c in s] #encoder: take a string, output a list of integer\n",
    "decode = lambda  l: ''.join([itos[i] for i in l]) # decode: takes a list of indexes and output a string\n",
    "\n",
    "\n",
    "# train and val split \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Length of train data: {len(train_data)}\")\n",
    "print(f\"Length of validation data: {len(val_data)}\")\n",
    "\n",
    "\n",
    "# get batch \n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Get a mini batch from specified data split\n",
    "    input: \"train\" or \"val\"\n",
    "    output: x, y \n",
    "    y is x moved to right by an index \n",
    "    x.shape -> [4,8]\n",
    "    y.shape -> [4,8]\n",
    "    \"\"\"\n",
    "    data = train_data if split ==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size-1,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move batch to device\n",
    "    return x,y\n",
    "\n",
    "# biagram language model \n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.Embedding -> A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # embed the identity of the token \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx,targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are of shape (B,T)\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T, n_embd)\n",
    "        position_embd = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
    "        x = token_embd + position_embd # (B,T,n_emd)\n",
    "        logits = self.lm_head(x) # B,T, vocab_size\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            # pytorch cross entropy expects second dimention as the channel\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is B,T array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context before feeding into self \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions \n",
    "            logits, loss = self(idx_cond) # (B, T, C) # self here runs the def forward \n",
    "            # get predictions for the last time step \n",
    "            logits = logits[:, -1, :] #(B,C) pluck the last time step\n",
    "            # apply soft max to get the probabilities \n",
    "            prob = F.softmax(logits, dim = -1) #(B, C)\n",
    "            # sample from the distibution \n",
    "            idx_next = torch.multinomial(prob, num_samples=1) # (B, 1)\n",
    "            # append the sampled index into the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim= 1) # (B,T+1)\n",
    "        return idx\n",
    "# define model \n",
    "model = TransformerLanguageModel()\n",
    "model = model.to(device)\n",
    "print(\"model created successfully!\")\n",
    "\n",
    "# create optimizer \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Evaluate loss \n",
    "@torch.no_grad()\n",
    "def evaluate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x,y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# training loop \n",
    "print(\"training started...\")\n",
    "for iter in range(max_iterations):\n",
    "    #once every once a while evalauate loss on the train adn eval set\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = evaluate_loss()\n",
    "        print(f\"step {iter}/ {max_iterations},  train_loss: {losses['train']:.4f}, val_loss: {losses['val']:.4f}\")\n",
    "    # sample from the batch\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # forward pass and evaluate the loss \n",
    "    logits, loss = model(xb,yb)\n",
    "    # backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    # update \n",
    "    optimizer.step()\n",
    "print(\"training complete!\")\n",
    "\n",
    "# generate sample text \n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens= 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4edd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 self attention \n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8, 32 \n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# single head of self attention\n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size) # Linear projection of channel to head_size (independent of query, value)\n",
    "query = nn.Linear(C, head_size) # Linear projection of channel to head_size (independent of key, value)\n",
    "value = nn.Linear(C, head_size) # Linear projection of channel to head_size (independent of key, query)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "#wei = torch.zeros((T, T))\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B,T,T)\n",
    "# wei -> torch.Size([4, 8, 8])\n",
    "# wei[0] now is: in a batch 1: we have a (T,T) matrix of affinities. \n",
    "# how have each token interacted with rest of the tokens \n",
    "\n",
    "# now we mask affinities from future\n",
    "tril = torch.tril(torch.ones(T,T)) #(T,T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #(B,T,T)\n",
    "# normalise them to get probability distributions\n",
    "wei = F.softmax(wei, dim =-1 ) #(B,T,T)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v # (B,T,T) @ (B,T,head_size) ==> (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "749f6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class implementation for self attention head \n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)) )\n",
    "\n",
    "    def forward(self,x):\n",
    "        B, T, C = x.shape\n",
    "        # k,q,v\n",
    "        k = self.key(x) #(B, T, head_size)\n",
    "        q = self.query(x) #(B, T, head_size)\n",
    "        # compute attention scores ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1)* head_size**-0.5 # (B, T, head_size) @ (B, head_size, T) --> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #(B,T,T)\n",
    "        wei = F.softmax(wei, dim =-1 ) #(B,T,T)\n",
    "        # perform weighted aggregation of values\n",
    "        v = value(x) #(B, T, head_size)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,head_size) ==> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class implemetnation fo multihead attention \n",
    "\n",
    "class MultiheadAttenion(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat( [h(x) for h in self.heads], dim = -1) # concatenate in channel dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class implementation for feed forward network \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fan_in, fan_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
